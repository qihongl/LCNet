# A incremental sticky CRP
#
# Note:
# This is modified based on John McDonnell's Anderson model
# https://github.com/johnmcdonnell/Rational-Models-of-Categorization/blob/master/rational.py
# References: Anderson (1990) and Anderson (1991)
#
# Implementation of Anderson's venerable "rational" model of categorization.
# Assumes that stimuli were generated by a mixture of Gaussian distributions;
# rather than compute the full Bayesian posterior, it views items sequentially
# and assigns each to the maximum a posteriori cluster.
#
# At the end it is presented with a stimulus with one item missing, and
# predicts the probability that its value is a '0' or a '1'.


import numpy as np
from random import shuffle


class dLocalMAP:
    """
    See Anderson (1990, 1991)
    'Categories' renamed 'clusters' to avoid confusion.
    Discrete version.

    Stimulus format is a list of integers from 0 to n-1 where n is the number
    of possible features (e.g. [1,0,1])

    Parameters
    ----------
    c : float in [0, 1]
        coupling probability, a fixed probability two objects come from the
        same category, and this probability does not depend on the number
        of objects seen so far.
        The greater the coupling probability and the more objects, the less
        likely it is that the new object comes from an entirely new category
        low c -> high p for new cluster
    alpha:
        strenght of belief in the priors, 1 if uninformative
    context_dim : int
        the dimension of the context vector -- a real valued vector
        representation of the cluster id
    lmbd int
        the stickiness level, used as a pseudo count

    Attributes
    ----------
    partition : type
        Description of attribute `partition`.
    alpha0 : type
        Description of attribute `alpha0`.
    N : type
        Description of attribute `N`.
    context : type
        Description of attribute `context`.
    add_context_vec : type
        Description of attribute `add_context_vec`.
    """

    def __init__(self, c, lmbd, alpha=None, context_dim=64, allow_redundant_instance=True):
        assert c >= 0 and c <= 1
        assert lmbd >= 0 and lmbd <= 1
        assert context_dim > 0
        # params
        self.c = c
        self.lmbd = lmbd
        self.context_dim = context_dim
        if alpha is None:
            self.alpha = np.ones((context_dim, 2))
        else:
            self.alpha = alpha
        self.alpha0 = sum(self.alpha.T)
        #
        self.allow_redundant_instance = allow_redundant_instance
        self.reinit()

    def reinit(self):
        # the partition over previously seen inputs
        self.context = []
        self.add_context_vec()
        self.N = 0
        self.partition = [[]]
        self.prev_k = None

    def probClustVal(self, k, i, val):
        '''
        Find P(j|k),
        the probability of an object from category k displaying value j on
        dimension i
        '''
        cj = len([x for x in self.partition[k] if x[i] == val])
        nk = len(self.partition)
        return (cj + self.alpha[i][val]) / (nk + self.alpha0[i])

    def condclusterprob(self, stim, k):
        '''Find P(F|k)'''
        pjks = []
        for i in range(len(stim)):
            # the # cluster-k instances that have matching feature at pos i
            cj = len([x for x in self.partition[k] if x[i] == stim[i]])
            # the count of cluster k
            nk = len(self.partition[k])
            # lik = n_matches + alpha / count + 2alpha
            # prop to n_matches
            pjks.append((cj + self.alpha[i][stim[i]]) / (nk + self.alpha0[i]))
        return np.product(pjks)

    def posterior(self, stim):
        '''
        Find P(k|F) for each cluster
        where P(k|F) is the probability that stim belongs to category k,
        given stim has feature F
        '''
        pk = np.zeros(len(self.partition))
        pFk = np.zeros(len(self.partition))
        # existing clusters:
        for k in range(len(self.partition)):
            pk[k] = self.c * len(self.partition[k]) / \
                ((1 - self.c) + self.c * self.N)
            if len(self.partition[k]) == 0:  # case of new cluster
                pk[k] = (1 - self.c) / ((1 - self.c) + self.c * self.N)
            pFk[k] = self.condclusterprob(stim, k)
        # put it together
        # print('prior sum: ', np.sum(pk), pk)
        pkF = (pk * pFk)  # / sum( pk*pFk )
        return pkF

    def stimulate(self, stim):
        '''
        Argmax of P(k|F) + P(0|F)
        P(0|F): probaility that the object comes from a new category
        '''
        # print(stim)
        # print(type(stim))
        assert len(stim) == self.context_dim, \
        f'context dim is {self.context_dim}, incompatible with the input with dim {len(stim)}'
        # whether to use the previous cluster
        if self.N > 0 and np.random.uniform() < self.lmbd:
            self.partition[self.prev_k].append(stim)
            self.N += 1
            return self.prev_k

        winner = np.argmax(self.posterior(stim))
        if len(self.partition[winner]) == 0:
            self.partition.append([])
            self.add_context_vec()
        if self.allow_redundant_instance:
            self.partition[winner].append(stim)
            self.N += 1
        else:
            if not check_array_in_list(stim, self.partition[winner]):
                self.partition[winner].append(stim)
                self.N += 1
        # return the cluster id and update the just-seen cluster id
        cluster_id = self.get_cluster_id(stim)
        self.prev_k = cluster_id
        return cluster_id

    def query(self, stimulus):
        '''Queried value should be -1'''
        qdim = -1
        for i in range(len(stimulus)):
            if stimulus[i] < 0:
                if qdim != -1:
                    raise (Exception, "ERROR: Multiple dimensions queried.")
                qdim = i

        self.N = sum([len(x) for x in self.partition])
        pkF = self.posterior(stimulus)
        pkF = pkF[:-1] / sum(pkF[:-1])  # eliminate `new cluster' prob
        pjF = np.array(
            [sum([pkF[k] * self.probClustVal(k, qdim, j)
                  for k in range(len(self.partition) - 1)])
             for j in range(len(self.alpha[qdim]))]
        )
        return pjF / sum(pjF)

    def get_cluster_id(self, stimulus):
        '''
        get the cluster id of a stimulus, return nan if not in any cluster
        '''
        for cluster_id, cluster in enumerate(self.partition):
            for s in cluster:
                if np.all(stimulus == s):
                    return cluster_id
        return np.nan

    def add_context_vec(self, loc=0, scale=1):
        '''
        sample a random vector to represent the k-th context
        this is useful because
        - random vectors are easy to get
        - random vectors are roughly orthogonal
        '''
        self.context.append(
            np.random.normal(loc=loc, scale=scale, size=(self.context_dim,))
        )


def check_array_in_list(input_array, array_list):
    for array_i in array_list:
        if np.all(input_array == array_i):
            return True
    return False


"""
Tests the Anderson's ratinal model using the Medin & Schaffer (1978) data.

This script will print out the probability that each item belongs to each
of the existing clusters or to a new cluster, and the model assign it to
the most likely cluster. To see that the model is working correctly, you
can follow along with Anderson (1991), which steps through in the same way.
"""
'''how to use'''
if __name__ == "__main__":

    np.random.seed(0)
    # Medin & Schaffer (1978)
    stims = [[1, 1, 1, 1, 1],
             [1, 0, 1, 0, 1],
             [1, 1, 1, 1, 1],
             [1, 1, 1, 1, 1],
             [1, 0, 1, 0, 1],
             [1, 0, 1, 0, 1]]
    # These are the classic Shepard Type II and Type IV datasets.
    # Uncomment the one you want to try out; you might want to uncomment
    # shuffling the stims too if you don't care about order.
    # Type IV
    # stims = [[0, 0, 0, 0], [0, 0, 1, 0], [1, 1, 0, 1], [1, 1, 1, 1],
    #          [1, 0, 0, 0], [1, 0, 1, 1], [0, 1, 0, 0], [0, 1, 1, 1]]
    # Type II
    # stims = [[0, 0, 0, 0], [0, 0, 1, 0], [1, 1, 0, 0], [1, 1, 1, 0],
    #          [1, 0, 0, 1], [1, 0, 1, 1], [0, 1, 0, 1], [0, 1, 1, 1]]

    input_dim = len(stims[0])
    c = .5
    lmbd = 1
    model = dLocalMAP(c, lmbd, context_dim=input_dim)

    # shuffle(stims)
    print(f'n context = {len(model.context)}')
    print("Partition: ", model.partition)
    print()
    for s in stims:

        cluster_id = model.stimulate(s)
        print(f"Stim: {s}\ncluster_id = {cluster_id}")
        print(f'n context = {len(model.context)}')
        print("Partition: ", model.partition)
        print(f'Cluster id = {model.get_cluster_id(s)}')
        print(f'N = {model.N}')
        # print(model.posterior(s))
        # print(model.context)
        # print(model.prev_k)
        print()
    # model.alpha0
    # print()
    # print("Prob vals for 0,0,0,0,?", model.query(
    #     [0] * (len(stims[0]) - 1) + [-1]))

# model.alpha0
